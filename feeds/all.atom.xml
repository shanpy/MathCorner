<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Pengyin Shan and Math, Finance, Statistics</title><link href="http://shanpy.github.io/mathcorner/" rel="alternate"></link><link href="http://shanpy.github.io/mathcorner/feeds/all.atom.xml" rel="self"></link><id>http://shanpy.github.io/mathcorner/</id><updated>2015-05-30T00:00:00-04:00</updated><entry><title>An Introduction to Think Stats - Chapter 1</title><link href="http://shanpy.github.io/mathcorner/an-introduction-to-think-stats-chapter-1.html" rel="alternate"></link><updated>2015-05-30T00:00:00-04:00</updated><author><name>Pengyin Shan</name></author><id>tag:shanpy.github.io,2015-01-13:mathcorner/an-introduction-to-think-stats-chapter-1.html</id><summary type="html">&lt;h1&gt;Chapter 1: Exploratory Data&amp;nbsp;Analysis&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Anecdotal Evidence&lt;/strong&gt; is the evidence based on the data that is unpublished and usually personal, rather than by a well-designed&amp;nbsp;study.&lt;/p&gt;
&lt;p&gt;Anecdotal Evidence usually fails because of four&amp;nbsp;reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Small Number of&amp;nbsp;Observations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Selection Bias: people who discuss the question may be interested in one aspects of the&amp;nbsp;answer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confirmation Bias: people who belive the claim might be more likely to contribute examples that confirm&amp;nbsp;it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inaccuracy: Anecdotes are ofter personal, so there exists misremembered, misrepeated,&amp;nbsp;etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To avoid Anecdotal Evidence, a few &lt;strong&gt;tools of statistics&lt;/strong&gt; can be&amp;nbsp;used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data Collection: large and trustable data&amp;nbsp;source&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Descriptive Statistics: statistics that summarize data concisely, maybe with data&amp;nbsp;virtualization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exploratory Data Analysis: patterns, differnces and other features that address the question. Inconsistency and Identify&amp;nbsp;limitations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimation: use data from sample to estimate&amp;nbsp;characteristics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hypothesis Testing: if there is any apparent effects, such as a difference between two groups, evaluate if this effect happening by&amp;nbsp;chance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Population&lt;/strong&gt;: a group we are interested in&amp;nbsp;studying.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cross-Sectional Study&lt;/strong&gt;: captures a snapshot of a group at a point in time. It is &lt;strong&gt;representative&lt;/strong&gt;, which means every number of the target population has an equal change of participating. &lt;strong&gt;Oversampled&lt;/strong&gt; is opposite to representative. It means some groups of population has much higher rate in all sample groups. Oversampling is used to avoid errors due to small sample&amp;nbsp;sizes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Longitudinal Study&lt;/strong&gt;: observes a group repeatedly over a period of&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cycle&lt;/strong&gt;: a survey can be conducted several times. Each deployment is called a&amp;nbsp;cycle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample&lt;/strong&gt;: data from a subset of&amp;nbsp;population&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Record&lt;/strong&gt;: In a dataset, a collection of information about a single person or other&amp;nbsp;subject.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Respondents&lt;/strong&gt;: people who participate in the&amp;nbsp;survey&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DataFrame&lt;/strong&gt;: the fundamental data structure provided by &lt;strong&gt;pandas&lt;/strong&gt;. It contains a row of each record, and the variable names and their types. It also provides methods of accessing and modifying&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Raw Data&lt;/strong&gt;: values collected with little or no checking, calculation or&amp;nbsp;interpretation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recodes&lt;/strong&gt;: instead of being part of raw data, recodes are calculated by raw data. Recodes are often based on logic that checks the consistency and accuracy of the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Cleaning&lt;/strong&gt;: operations such as check for errors, deal with sepecial values, convert data into different formats and perform calculations after you import&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Validation&lt;/strong&gt;: one way to validate data is to compute basic statistics and compare them with published&amp;nbsp;results&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Interpretation&lt;/strong&gt;: To work with data effectively, you have to think on two levels at the same time: the level of &lt;strong&gt;statistics&lt;/strong&gt; and the level of &lt;strong&gt;context&lt;/strong&gt;. Some result is statistically acceptable but not natual in&amp;nbsp;context.&lt;/p&gt;</summary><category term="Statistics"></category><category term="Data Science"></category></entry><entry><title>Probabilistic Programming and Bayesian Methods for Hackers Reading Note - Chapter 1</title><link href="http://shanpy.github.io/mathcorner/probabilistic-programming-and-bayesian-methods-for-hacker-reading-note-chapter-1.html" rel="alternate"></link><updated>2015-05-30T00:00:00-04:00</updated><author><name>Pengyin Shan</name></author><id>tag:shanpy.github.io,2015-01-13:mathcorner/probabilistic-programming-and-bayesian-methods-for-hacker-reading-note-chapter-1.html</id><summary type="html">&lt;p&gt;This post is being transferred from &lt;a href="shanpy.github.io/techblog"&gt;my old blog site&lt;/a&gt;, about Python development for&amp;nbsp;statistics.&lt;/p&gt;
&lt;h2&gt;The Philosophy of Bayesian&amp;nbsp;Inference&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bayesian inference&lt;/strong&gt; is simply updating your &lt;strong&gt;beliefs&lt;/strong&gt; after considering new evidence. We update our beliefs about an outcome; ralely can we be absolutely sure unless we rule out all other&amp;nbsp;alternatives.&lt;/p&gt;
&lt;p&gt;In Bayesian, the &lt;strong&gt;probability&lt;/strong&gt; is consideres as &lt;strong&gt;believablity&lt;/strong&gt; of an event. i.e. how &lt;strong&gt;confident&lt;/strong&gt; we&amp;nbsp;are.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frequentist&lt;/strong&gt;: assume probablity is the &lt;strong&gt;long-run&lt;/strong&gt; frequency of&amp;nbsp;events. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayesian&lt;/strong&gt;: asssume probablity is a measure of &lt;strong&gt;belief&lt;/strong&gt;. Note that belief is assigneed &lt;strong&gt;individually&lt;/strong&gt;, which means there can be &lt;strong&gt;conflicting&lt;/strong&gt; of&amp;nbsp;blieves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P(A)&lt;/strong&gt;: prior&amp;nbsp;probability&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P(A|X)&lt;/strong&gt;: the probablity of A given the evidence &lt;code&gt;X&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For example: &lt;code&gt;P(A)&lt;/code&gt;: The code likely has a bug with it. &lt;code&gt;P(A|X)&lt;/code&gt;: The code passed all &lt;code&gt;X&lt;/code&gt; tests, there still might be a bug, but its presence is less likely&amp;nbsp;now&lt;/p&gt;
&lt;p&gt;Every time after a new evidence &lt;code&gt;X&lt;/code&gt; comes, we &lt;strong&gt;re-weighted&lt;/strong&gt; the &lt;strong&gt;prior probablity&lt;/strong&gt; to incorporated the new evidence. After this process, our guesses become &lt;strong&gt;less wrong&lt;/strong&gt;. i.e. we try to be more &lt;strong&gt;right&lt;/strong&gt; after each&amp;nbsp;guess.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frequentist&lt;/strong&gt; will return a number, while Bayesians will return&amp;nbsp;probablity. &lt;/p&gt;
&lt;p&gt;Forthe example above, if asking &amp;#8220;My code passes all tests. Is my code bug-free?&amp;#8221;, Frequentist will return&amp;nbsp;yes. &lt;/p&gt;
&lt;p&gt;If asking &amp;#8220;&lt;em&gt;Often my code has bugs. (prior parameter)&lt;/em&gt; My code passed all X tests. Is my code bug-free?&amp;#8221;, Bayesians will return &amp;#8220;Yes, with probablity 0.8; No, with probablity&amp;nbsp;0.2&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Denote &lt;code&gt;N&lt;/code&gt; as the number of &lt;strong&gt;instances of evidence&lt;/strong&gt; we possess. As we gather an &lt;strong&gt;infinite&lt;/strong&gt; amount of evidence, say as &lt;code&gt;N -&amp;gt; Infinity&lt;/code&gt;, our Bayesian results (often) align with frequentist&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;So for large &lt;code&gt;N&lt;/code&gt;, statistical inference is more or less &lt;strong&gt;objective&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For small &lt;code&gt;N&lt;/code&gt;, inference is much more &lt;strong&gt;unstable&lt;/strong&gt;: Frequentist estimates have more variance and larger confidence&amp;nbsp;intervals.&lt;/p&gt;
&lt;p&gt;Hence Bayesians introduce a &lt;strong&gt;prior&lt;/strong&gt; and returning &lt;strong&gt;probablities&lt;/strong&gt;, it can &lt;strong&gt;preserve&lt;/strong&gt; the &lt;strong&gt;uncentainty&lt;/strong&gt; that reflects the instability (not stable) of statistical inference of a small &lt;code&gt;N&lt;/code&gt; dataset.&lt;/p&gt;
&lt;h3&gt;Bayes&amp;#8217;&amp;nbsp;Theorem&lt;/h3&gt;
&lt;p&gt;if our code passes &lt;code&gt;X&lt;/code&gt; tests, we want to &lt;strong&gt;update&lt;/strong&gt; our belief to incorporate this. We call this new belief the &lt;strong&gt;posterior probability&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;Bayes&amp;#8217; Theorem is used for updating&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;Bayesian inference merely uses it to connect &lt;strong&gt;prior probabilities&lt;/strong&gt; &lt;code&gt;P(A)&lt;/code&gt; with an updated &lt;strong&gt;posterior probabilities&lt;/strong&gt; &lt;code&gt;P(A|X)&lt;/code&gt; .&lt;/p&gt;
&lt;h3&gt;The Understanding of Variables for Coding&amp;nbsp;Example&lt;/h3&gt;
&lt;p&gt;Assume &lt;code&gt;A&lt;/code&gt; means the event that code has &lt;strong&gt;no&lt;/strong&gt; bug and &lt;code&gt;X&lt;/code&gt; means code passes &lt;code&gt;X&lt;/code&gt; tests. Assume &lt;code&gt;P(A) = p&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P(A|X)&lt;/strong&gt;: the probability for no bugs, giving passing debug test &lt;code&gt;X&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P(X|A)&lt;/strong&gt;: the probability that code passes &lt;code&gt;X&lt;/code&gt; test given there is no bugs. This always be&amp;nbsp;1.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#Since We can get P(X) by following way:&lt;/span&gt;

&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;|~&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;|~&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;#Assume P(X|~A) = 0.5 here, and we know P(X|A) = 1  &lt;/span&gt;
&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c"&gt;#This is the posterior probability&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Probability&amp;nbsp;Distributions&lt;/h2&gt;
&lt;p&gt;Let &lt;code&gt;Z&lt;/code&gt; be some random variable. Then associated with &lt;code&gt;Z&lt;/code&gt; is a &lt;strong&gt;probability distribution function&lt;/strong&gt; that assigns probablities to the different &lt;strong&gt;outcomes&lt;/strong&gt; &lt;code&gt;Z&lt;/code&gt; can take. A &lt;strong&gt;probability distribution&lt;/strong&gt; is a curve where the probability of an outcome is &lt;em&gt;proportional&lt;/em&gt; to the &lt;em&gt;height&lt;/em&gt; of the&amp;nbsp;curve.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Z&lt;/code&gt; can be discreate: only assume values on a speical list. Such as populations, movie ratings,&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Z&lt;/code&gt; can be continous: takes aribitrarily exact values. Such as temporature, speed,&amp;nbsp;etc&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Z&lt;/code&gt; can be mixed by two types&amp;nbsp;above.&lt;/p&gt;
&lt;h3&gt;If Z is&amp;nbsp;discreate&lt;/h3&gt;
&lt;p&gt;If &lt;code&gt;Z&lt;/code&gt; is discreate, its distribution is called &lt;strong&gt;probability mass function&lt;/strong&gt;, which measures the probablity &lt;code&gt;Z&lt;/code&gt; takes on the value &lt;code&gt;k&lt;/code&gt;, denoted &lt;code&gt;P(Z=k)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We saw &lt;code&gt;Z&lt;/code&gt; is &lt;strong&gt;posisson-distributed&lt;/strong&gt;&amp;nbsp;if: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;λ&lt;/code&gt; is called a parameter of the distribution, or &lt;strong&gt;intensity&lt;/strong&gt; of the &lt;strong&gt;possison distribution&lt;/strong&gt; and it controls the distribution&amp;#8217;s &lt;strong&gt;shape&lt;/strong&gt;. Here, &lt;code&gt;λ&lt;/code&gt; can be any &lt;em&gt;positive&lt;/em&gt;&amp;nbsp;number.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if &lt;code&gt;λ&lt;/code&gt; increate, add probability to larger values. if &lt;code&gt;λ&lt;/code&gt; decrease, add probability to smaller&amp;nbsp;values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;k&lt;/code&gt; must be a &lt;strong&gt;non-negative&lt;/strong&gt;&amp;nbsp;integer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If a random variable &lt;code&gt;Z&lt;/code&gt; has a &lt;strong&gt;poisson mass distribution&lt;/strong&gt;, we can express as: &lt;code&gt;Z ~ Poi(λ)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For a poisson distribution, its expected value is equal to its parameter, i.e. &lt;code&gt;E[Z | λ] = λ&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;If Z is&amp;nbsp;continuous&lt;/h3&gt;
&lt;p&gt;If &lt;code&gt;Z&lt;/code&gt; is continous, its distribution is called &lt;strong&gt;probability density function&lt;/strong&gt;. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;z&lt;/code&gt; can only take &lt;strong&gt;non-negative&lt;/strong&gt; value, including&amp;nbsp;non-integer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If &lt;code&gt;Z&lt;/code&gt; has an exponential distribution, we say &lt;code&gt;Z&lt;/code&gt; is &lt;strong&gt;exponential&lt;/strong&gt; and we have: &lt;code&gt;Z ~ Exp(λ)&lt;/code&gt; and &lt;code&gt;E[Z | λ] = 1/λ&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#PyMC&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pymc&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pm&lt;/span&gt;

&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;count_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;lambda_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exponential&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lambda_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;#get lambda value&lt;/span&gt;
&lt;span class="n"&gt;lambda_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exponential&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lambda_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;tau&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DistcreateUniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tau&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_count_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="Statistics"></category><category term="Python"></category><category term="Data Science"></category></entry></feed>